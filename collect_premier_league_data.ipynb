{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58469a48",
   "metadata": {},
   "source": [
    "# Premier League Data Collection for RAG Training\n",
    "\n",
    "This notebook downloads Premier League history articles from Wikipedia and other sources to create a comprehensive dataset for RAG (Retrieval-Augmented Generation) training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a133b",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "Install necessary libraries for web scraping and Wikipedia API access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b02c558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\my projects\\bizmind\\venv\\lib\\site-packages (2.32.5)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.14.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting wikipedia-api\n",
      "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting lxml\n",
      "  Downloading lxml-6.0.2-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: tqdm in d:\\my projects\\bizmind\\venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\my projects\\bizmind\\venv\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\my projects\\bizmind\\venv\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\my projects\\bizmind\\venv\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\my projects\\bizmind\\venv\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Collecting soupsieve>=1.6.1 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\my projects\\bizmind\\venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: colorama in d:\\my projects\\bizmind\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n",
      "Downloading lxml-6.0.2-cp311-cp311-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/4.0 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.3/4.0 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.1/4.0 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.1/4.0 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.9/4.0 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 3.9 MB/s  0:00:01\n",
      "Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: wikipedia-api\n",
      "  Building wheel for wikipedia-api (pyproject.toml): started\n",
      "  Building wheel for wikipedia-api (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for wikipedia-api: filename=wikipedia_api-0.8.1-py3-none-any.whl size=15542 sha256=df6fbfe761bab320bbbbfe71353c6fe49fe0a22dcd81e9aa8587a4d109fe8659\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\0b\\0f\\39\\e8214ec038ccd5aeb8c82b957289f2f3ab2251febeae5c2860\n",
      "Successfully built wikipedia-api\n",
      "Installing collected packages: soupsieve, lxml, wikipedia-api, beautifulsoup4\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [lxml]\n",
      "   ---------------------------------------- 4/4 [beautifulsoup4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.14.3 lxml-6.0.2 soupsieve-2.8 wikipedia-api-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 wikipedia-api lxml tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5876047",
   "metadata": {},
   "source": [
    "## 2. Import Dependencies\n",
    "\n",
    "Import all necessary libraries for the data collection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac81810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipediaapi\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f41193",
   "metadata": {},
   "source": [
    "## 3. Set Up Directory Structure\n",
    "\n",
    "Create organized subdirectories for storing articles by source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75459d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Directory structure created in 'premier_league_data/'\n",
      "  - premier_league_data/wikipedia/\n",
      "  - premier_league_data/metadata/\n"
     ]
    }
   ],
   "source": [
    "# Set up directory structure\n",
    "base_dir = \"premier_league_data\"\n",
    "subdirs = [\"wikipedia\", \"metadata\"]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "for subdir in subdirs:\n",
    "    os.makedirs(os.path.join(base_dir, subdir), exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Directory structure created in '{base_dir}/'\")\n",
    "print(f\"  - {base_dir}/wikipedia/\")\n",
    "print(f\"  - {base_dir}/metadata/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916fa2bc",
   "metadata": {},
   "source": [
    "## 4. Download Wikipedia Articles\n",
    "\n",
    "Download comprehensive articles about Premier League history, teams, players, and seasons from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9988f888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to download 73 Wikipedia articles...\n"
     ]
    }
   ],
   "source": [
    "# Initialize Wikipedia API with user agent\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language='en',\n",
    "    user_agent='BizMind RAG Training Bot/1.0 (Educational Purpose)'\n",
    ")\n",
    "\n",
    "# Comprehensive list of Premier League related Wikipedia articles\n",
    "wikipedia_articles = [\n",
    "    # General Premier League\n",
    "    \"Premier League\",\n",
    "    \"History of the Premier League\",\n",
    "    \"Premier League records and statistics\",\n",
    "    \"List of Premier League seasons\",\n",
    "    \"Premier League Golden Boot\",\n",
    "    \"Premier League Player of the Season\",\n",
    "    \n",
    "    # Historic Seasons\n",
    "    \"1992â€“93 FA Premier League\",\n",
    "    \"1993â€“94 FA Premier League\",\n",
    "    \"1994â€“95 FA Premier League\",\n",
    "    \"1995â€“96 FA Premier League\",\n",
    "    \"1996â€“97 FA Premier League\",\n",
    "    \"1997â€“98 FA Premier League\",\n",
    "    \"1998â€“99 FA Premier League\",\n",
    "    \"1999â€“2000 FA Premier League\",\n",
    "    \"2000â€“01 FA Premier League\",\n",
    "    \"2001â€“02 FA Premier League\",\n",
    "    \"2002â€“03 FA Premier League\",\n",
    "    \"2003â€“04 FA Premier League\",\n",
    "    \"2004â€“05 FA Premier League\",\n",
    "    \"2005â€“06 Premier League\",\n",
    "    \"2006â€“07 Premier League\",\n",
    "    \"2007â€“08 Premier League\",\n",
    "    \"2008â€“09 Premier League\",\n",
    "    \"2009â€“10 Premier League\",\n",
    "    \"2010â€“11 Premier League\",\n",
    "    \"2011â€“12 Premier League\",\n",
    "    \"2012â€“13 Premier League\",\n",
    "    \"2013â€“14 Premier League\",\n",
    "    \"2014â€“15 Premier League\",\n",
    "    \"2015â€“16 Premier League\",\n",
    "    \"2016â€“17 Premier League\",\n",
    "    \"2017â€“18 Premier League\",\n",
    "    \"2018â€“19 Premier League\",\n",
    "    \"2019â€“20 Premier League\",\n",
    "    \"2020â€“21 Premier League\",\n",
    "    \"2021â€“22 Premier League\",\n",
    "    \"2022â€“23 Premier League\",\n",
    "    \"2023â€“24 Premier League\",\n",
    "    \"2024â€“25 Premier League\",\n",
    "    \n",
    "    # Big Six Clubs\n",
    "    \"Manchester United F.C.\",\n",
    "    \"Liverpool F.C.\",\n",
    "    \"Arsenal F.C.\",\n",
    "    \"Chelsea F.C.\",\n",
    "    \"Manchester City F.C.\",\n",
    "    \"Tottenham Hotspur F.C.\",\n",
    "    \n",
    "    # Other Notable Clubs\n",
    "    \"Leicester City F.C.\",\n",
    "    \"Everton F.C.\",\n",
    "    \"Newcastle United F.C.\",\n",
    "    \"Aston Villa F.C.\",\n",
    "    \"West Ham United F.C.\",\n",
    "    \n",
    "    # Legendary Players\n",
    "    \"Thierry Henry\",\n",
    "    \"Alan Shearer\",\n",
    "    \"Ryan Giggs\",\n",
    "    \"Frank Lampard\",\n",
    "    \"Steven Gerrard\",\n",
    "    \"Wayne Rooney\",\n",
    "    \"Cristiano Ronaldo\",\n",
    "    \"Sergio AgÃ¼ero\",\n",
    "    \"Mohamed Salah\",\n",
    "    \"Kevin De Bruyne\",\n",
    "    \"Eric Cantona\",\n",
    "    \"Dennis Bergkamp\",\n",
    "    \"Didier Drogba\",\n",
    "    \"Patrick Vieira\",\n",
    "    \"Roy Keane\",\n",
    "    \n",
    "    # Legendary Managers\n",
    "    \"Alex Ferguson\",\n",
    "    \"ArsÃ¨ne Wenger\",\n",
    "    \"JosÃ© Mourinho\",\n",
    "    \"Pep Guardiola\",\n",
    "    \"JÃ¼rgen Klopp\",\n",
    "    \n",
    "    # Notable Events\n",
    "    \"Treble (association football)\",\n",
    "    \"Invincibles (English football)\",\n",
    "    \"2015â€“16 Leicester City F.C. season\",\n",
    "]\n",
    "\n",
    "print(f\"Preparing to download {len(wikipedia_articles)} Wikipedia articles...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52043be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Wikipedia articles:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 71/73 [01:35<00:04,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âš  Article not found: Invincibles (English football)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Wikipedia articles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [01:37<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Successfully downloaded 72 articles\n",
      "âœ— Failed to download 1 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download Wikipedia articles\n",
    "downloaded_articles = []\n",
    "failed_articles = []\n",
    "\n",
    "for article_title in tqdm(wikipedia_articles, desc=\"Downloading Wikipedia articles\"):\n",
    "    try:\n",
    "        # Fetch the article\n",
    "        page = wiki_wiki.page(article_title)\n",
    "        \n",
    "        if page.exists():\n",
    "            # Clean the filename\n",
    "            filename = re.sub(r'[^\\w\\s-]', '', article_title).strip().replace(' ', '_')\n",
    "            filename = f\"{filename}.txt\"\n",
    "            filepath = os.path.join(base_dir, \"wikipedia\", filename)\n",
    "            \n",
    "            # Save the article content\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"Title: {page.title}\\n\")\n",
    "                f.write(f\"URL: {page.fullurl}\\n\")\n",
    "                f.write(f\"Downloaded: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                f.write(\"=\"*80 + \"\\n\\n\")\n",
    "                f.write(page.text)\n",
    "            \n",
    "            downloaded_articles.append({\n",
    "                \"title\": page.title,\n",
    "                \"filename\": filename,\n",
    "                \"url\": page.fullurl,\n",
    "                \"word_count\": len(page.text.split()),\n",
    "                \"downloaded_at\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "        else:\n",
    "            failed_articles.append(article_title)\n",
    "            print(f\"\\nâš  Article not found: {article_title}\")\n",
    "        \n",
    "        # Be respectful to Wikipedia servers\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed_articles.append(article_title)\n",
    "        print(f\"\\nâœ— Error downloading '{article_title}': {str(e)}\")\n",
    "\n",
    "print(f\"\\nâœ“ Successfully downloaded {len(downloaded_articles)} articles\")\n",
    "print(f\"âœ— Failed to download {len(failed_articles)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010da5cf",
   "metadata": {},
   "source": [
    "## 5. Scrape Premier League Official Website\n",
    "\n",
    "Scrape historical content from the official Premier League website (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e98638c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Official website scraping function defined. Requires adaptation to current site structure.\n"
     ]
    }
   ],
   "source": [
    "# Note: This is a basic scraper example. The actual implementation would need\n",
    "# to be adapted based on the current structure of premierleague.com\n",
    "\n",
    "def scrape_premier_league_history():\n",
    "    \"\"\"\n",
    "    Scrape Premier League history pages\n",
    "    Note: This function needs to be adapted based on the current website structure\n",
    "    \"\"\"\n",
    "    # Base URL\n",
    "    base_url = \"https://www.premierleague.com\"\n",
    "    \n",
    "    # Headers to mimic browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    # Example: Scraping history page (this URL might need updating)\n",
    "    urls_to_scrape = [\n",
    "        f\"{base_url}/history\",\n",
    "    ]\n",
    "    \n",
    "    scraped_content = []\n",
    "    \n",
    "    for url in urls_to_scrape:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract main content (this selector needs to match actual site structure)\n",
    "            # This is a generic example\n",
    "            main_content = soup.find('main') or soup.find('article') or soup.find('body')\n",
    "            \n",
    "            if main_content:\n",
    "                text = main_content.get_text(separator='\\n', strip=True)\n",
    "                \n",
    "                # Save content\n",
    "                filename = \"premier_league_history.txt\"\n",
    "                filepath = os.path.join(base_dir, \"wikipedia\", filename)\n",
    "                \n",
    "                with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"Source: {url}\\n\")\n",
    "                    f.write(f\"Downloaded: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "                    f.write(text)\n",
    "                \n",
    "                scraped_content.append({\n",
    "                    \"source\": \"Premier League Official\",\n",
    "                    \"url\": url,\n",
    "                    \"filename\": filename\n",
    "                })\n",
    "                \n",
    "            time.sleep(2)  # Respectful rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {str(e)}\")\n",
    "    \n",
    "    return scraped_content\n",
    "\n",
    "# Uncomment to run (requires proper website structure analysis first)\n",
    "# official_content = scrape_premier_league_history()\n",
    "print(\"Note: Official website scraping function defined. Requires adaptation to current site structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca7d6c",
   "metadata": {},
   "source": [
    "## 6. Scrape Additional Sports News Sites\n",
    "\n",
    "Collect articles from sports news websites (with rate limiting and respect for robots.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98200d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Additional scraping functions defined but require proper implementation.\n"
     ]
    }
   ],
   "source": [
    "# Example function for scraping news articles\n",
    "# Note: Always check robots.txt and terms of service before scraping\n",
    "\n",
    "def scrape_bbc_sport_pl_history():\n",
    "    \"\"\"\n",
    "    Example function to scrape BBC Sport Premier League history\n",
    "    This is for educational purposes and would need proper implementation\n",
    "    \"\"\"\n",
    "    url = \"https://www.bbc.com/sport/football/premier-league\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # This is a placeholder - actual implementation needs proper selectors\n",
    "        print(f\"âœ“ Successfully connected to {url}\")\n",
    "        print(\"Note: Full implementation requires proper HTML structure analysis\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Uncomment to run (requires proper implementation)\n",
    "# scrape_bbc_sport_pl_history()\n",
    "print(\"Note: Additional scraping functions defined but require proper implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c20211f",
   "metadata": {},
   "source": [
    "## 7. Clean and Format Article Text\n",
    "\n",
    "Process the scraped content to ensure consistent formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea6b992f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning articles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72/72 [00:00<00:00, 598.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Cleaned 72 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text content\"\"\"\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', text)\n",
    "    # Remove excessive spaces\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    # Remove special characters that might cause encoding issues\n",
    "    text = text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "    return text.strip()\n",
    "\n",
    "def format_article(filepath):\n",
    "    \"\"\"Format and clean an article file\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        cleaned_content = clean_text(content)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(cleaned_content)\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error formatting {filepath}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Clean all downloaded articles\n",
    "wikipedia_dir = os.path.join(base_dir, \"wikipedia\")\n",
    "article_files = [f for f in os.listdir(wikipedia_dir) if f.endswith('.txt')]\n",
    "\n",
    "cleaned_count = 0\n",
    "for article_file in tqdm(article_files, desc=\"Cleaning articles\"):\n",
    "    filepath = os.path.join(wikipedia_dir, article_file)\n",
    "    if format_article(filepath):\n",
    "        cleaned_count += 1\n",
    "\n",
    "print(f\"\\nâœ“ Cleaned {cleaned_count} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c535b48",
   "metadata": {},
   "source": [
    "## 8. Save Articles to Local Files\n",
    "\n",
    "All articles have been saved during the download process with proper naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fa0d947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Total articles saved: 72\n",
      "âœ“ Location: d:\\My Projects\\BizMind\\premier_league_data\\wikipedia\n",
      "\n",
      "Sample filenames:\n",
      "  - 199293_FA_Premier_League.txt (10.9 KB)\n",
      "  - 199394_FA_Premier_League.txt (6.3 KB)\n",
      "  - 199495_FA_Premier_League.txt (6.8 KB)\n",
      "  - 199596_FA_Premier_League.txt (5.8 KB)\n",
      "  - 199697_FA_Premier_League.txt (3.8 KB)\n"
     ]
    }
   ],
   "source": [
    "# Verify all saved files\n",
    "wikipedia_dir = os.path.join(base_dir, \"wikipedia\")\n",
    "saved_files = [f for f in os.listdir(wikipedia_dir) if f.endswith('.txt')]\n",
    "\n",
    "print(f\"âœ“ Total articles saved: {len(saved_files)}\")\n",
    "print(f\"âœ“ Location: {os.path.abspath(wikipedia_dir)}\")\n",
    "\n",
    "# Show sample filenames\n",
    "print(\"\\nSample filenames:\")\n",
    "for filename in saved_files[:5]:\n",
    "    filepath = os.path.join(wikipedia_dir, filename)\n",
    "    file_size = os.path.getsize(filepath) / 1024  # Size in KB\n",
    "    print(f\"  - {filename} ({file_size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ae6e2",
   "metadata": {},
   "source": [
    "## 9. Create Metadata Index\n",
    "\n",
    "Generate a comprehensive index file with metadata for all downloaded articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "653e1919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Metadata saved to premier_league_data\\metadata\\articles_index.json\n",
      "âœ“ CSV index saved to premier_league_data\\metadata\\articles_index.csv\n"
     ]
    }
   ],
   "source": [
    "# Save metadata as JSON\n",
    "metadata_file = os.path.join(base_dir, \"metadata\", \"articles_index.json\")\n",
    "\n",
    "metadata_export = {\n",
    "    \"collection_date\": datetime.now().isoformat(),\n",
    "    \"total_articles\": len(downloaded_articles),\n",
    "    \"articles\": downloaded_articles,\n",
    "    \"failed_articles\": failed_articles\n",
    "}\n",
    "\n",
    "with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata_export, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ“ Metadata saved to {metadata_file}\")\n",
    "\n",
    "# Also save as a simpler CSV format for easy viewing\n",
    "import csv\n",
    "\n",
    "csv_file = os.path.join(base_dir, \"metadata\", \"articles_index.csv\")\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    if downloaded_articles:\n",
    "        writer = csv.DictWriter(f, fieldnames=downloaded_articles[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(downloaded_articles)\n",
    "\n",
    "print(f\"âœ“ CSV index saved to {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc2466b",
   "metadata": {},
   "source": [
    "## 10. Verify Downloaded Data\n",
    "\n",
    "Create a comprehensive summary report of all collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c7eaee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREMIER LEAGUE DATA COLLECTION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Collection Statistics:\n",
      "  â€¢ Total articles downloaded: 72\n",
      "  â€¢ Failed downloads: 1\n",
      "  â€¢ Total word count: 351,883\n",
      "  â€¢ Total data size: 2.06 MB\n",
      "  â€¢ Average words per article: 4,887\n",
      "\n",
      "ðŸ“ Data Location:\n",
      "  â€¢ Articles: d:\\My Projects\\BizMind\\premier_league_data\\wikipedia\n",
      "  â€¢ Metadata: d:\\My Projects\\BizMind\\premier_league_data\\metadata\n",
      "\n",
      "ðŸ“ Content Categories:\n",
      "  â€¢ General PL: 39 articles\n",
      "  â€¢ Seasons: 1 articles\n",
      "  â€¢ Clubs: 11 articles\n",
      "  â€¢ Players: 7 articles\n",
      "  â€¢ Managers: 5 articles\n",
      "  â€¢ Other: 9 articles\n",
      "\n",
      "âœ“ Data collection complete and ready for RAG training!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive summary report\n",
    "total_words = sum(article['word_count'] for article in downloaded_articles)\n",
    "total_size = sum(\n",
    "    os.path.getsize(os.path.join(base_dir, \"wikipedia\", article['filename']))\n",
    "    for article in downloaded_articles\n",
    ") / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PREMIER LEAGUE DATA COLLECTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ“Š Collection Statistics:\")\n",
    "print(f\"  â€¢ Total articles downloaded: {len(downloaded_articles)}\")\n",
    "print(f\"  â€¢ Failed downloads: {len(failed_articles)}\")\n",
    "print(f\"  â€¢ Total word count: {total_words:,}\")\n",
    "print(f\"  â€¢ Total data size: {total_size:.2f} MB\")\n",
    "print(f\"  â€¢ Average words per article: {total_words // len(downloaded_articles) if downloaded_articles else 0:,}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Data Location:\")\n",
    "print(f\"  â€¢ Articles: {os.path.abspath(os.path.join(base_dir, 'wikipedia'))}\")\n",
    "print(f\"  â€¢ Metadata: {os.path.abspath(os.path.join(base_dir, 'metadata'))}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Content Categories:\")\n",
    "categories = {\n",
    "    \"General PL\": 0,\n",
    "    \"Seasons\": 0,\n",
    "    \"Clubs\": 0,\n",
    "    \"Players\": 0,\n",
    "    \"Managers\": 0,\n",
    "    \"Other\": 0\n",
    "}\n",
    "\n",
    "for article in downloaded_articles:\n",
    "    title = article['title'].lower()\n",
    "    if any(x in title for x in ['premier league', 'history', 'records', 'golden boot']):\n",
    "        categories[\"General PL\"] += 1\n",
    "    elif any(x in title for x in ['season', 'â€“']):\n",
    "        categories[\"Seasons\"] += 1\n",
    "    elif any(x in title for x in ['f.c.', 'united', 'city', 'villa']):\n",
    "        categories[\"Clubs\"] += 1\n",
    "    elif any(x in title for x in ['henry', 'shearer', 'giggs', 'lampard', 'rooney', 'salah', 'agÃ¼ero']):\n",
    "        categories[\"Players\"] += 1\n",
    "    elif any(x in title for x in ['ferguson', 'wenger', 'mourinho', 'guardiola', 'klopp']):\n",
    "        categories[\"Managers\"] += 1\n",
    "    else:\n",
    "        categories[\"Other\"] += 1\n",
    "\n",
    "for category, count in categories.items():\n",
    "    if count > 0:\n",
    "        print(f\"  â€¢ {category}: {count} articles\")\n",
    "\n",
    "print(\"\\nâœ“ Data collection complete and ready for RAG training!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ae003",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The downloaded articles are now ready for RAG training:\n",
    "\n",
    "1. **Load into Vector Database**: Use the articles in the `premier_league_data/wikipedia/` folder to populate your vector database (ChromaDB, as seen in your project).\n",
    "\n",
    "2. **Create Embeddings**: Process the articles through your embedding model to create vector representations.\n",
    "\n",
    "3. **Configure RAG**: Update your `rag_mind/rag_model.py` to point to this new data source.\n",
    "\n",
    "4. **Test Queries**: Try asking questions about Premier League history to validate the RAG system.\n",
    "\n",
    "Example questions to test:\n",
    "- \"Who won the Premier League in 1992-93?\"\n",
    "- \"Tell me about the Invincibles season\"\n",
    "- \"Who has scored the most Premier League goals?\"\n",
    "- \"What was special about Leicester City in 2015-16?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
